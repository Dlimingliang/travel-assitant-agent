from openai import OpenAI
# mainæµ‹è¯•æ—¶ä½¿ç”¨
#from backend.app.config import get_settings
from backend.app.config import get_settings

# å…¨å±€LLMå®ä¾‹
_llm_instance = None

class LlmClient:
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = 30):
        self.model = model
        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)


    def chat(self, messages: list[dict[str, str]], temperature: int = 0):
        """
         è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚ è¿™é‡Œåªæä¾›éæµå¼,æµå¼å¦å¤–æä¾›æ–¹æ³•
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            print(f"è°ƒç”¨æ¨¡å‹ RequestJson: {messages}")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=False)
            print(f"è°ƒç”¨æ¨¡å‹ ResponseJson: {response.model_dump_json()}")
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")

def get_llm() -> LlmClient:
    """
    è·å–LLMå®ä¾‹(å•ä¾‹æ¨¡å¼)
    Returns:
       LlmClientå®ä¾‹
    """
    global _llm_instance

    if _llm_instance is None:
        settings = get_settings()
        _llm_instance = LlmClient(
            model=settings.llm_model,
            apiKey=settings.llm_api_key,
            baseUrl=settings.llm_base_url)

        print(f"âœ… LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    return _llm_instance

if __name__ == '__main__':
    llm_client = get_llm()
    messages = [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ]
    llm_client.chat(messages=messages)